{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "최근 자연어 처리 커뮤니티에서는 트랜스포머 기반의 여러 사전 학습모델을 묶어 만든 앙상블(ensemble) 모델이\n",
    "\n",
    "뛰어난 성능을 보여준다.\n",
    "\n",
    "질의응답 직업 벤치마크 결과를 섞어 만든 SQuAD2.0에 최고 성능이 모델 10개가 모두 앙상블 모델일 정도입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [ \n",
    "    ['What music do you like?', 'I like Rock music.', 1],\n",
    "    ['What is your favorite food?', 'I like sushi the best', 1],\n",
    "    ['What is your favorite color?', \"I'm going to be a doctor\", 0],\n",
    "    ['What is your favorite song?', \"Tokyo olympic game in 2020 was postponed\", 0],\n",
    "    ['Do you like watching TV shows?', \"Yeah, I often watch it in my spear time\", 1]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/workspace/colab_try/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertPreTrainedModel, BertConfig, BertModel, BertTokenizer, AdamW\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "class BertEnsembleForNextSentencePrediction(BertPreTrainedModel):\n",
    "    \"\"\"\n",
    "    BERT 앙상블 모델을 사용한 Next Sentence Prediction(NSP).\n",
    "    동일한 BERT 모델을 두 개 사용하여 서로 다른 입력을 처리한 후,\n",
    "    두 개의 출력 벡터를 연결 (concatenation)하여 최종 예측을 수행.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, *args, **kwargs):\n",
    "        super().__init__(config)\n",
    "\n",
    "        # 두 개의 동일한 BERT 모델을 초기화 (독립적인 가중치 설정)\n",
    "        self.bert_model_1 = BertModel(config)\n",
    "        self.bert_model_2 = BertModel(config)\n",
    "\n",
    "        # 두 BERT 모델의 출력(hidden state)을 겨합한 후, 이진 분류를 수행  output_layer = 2\n",
    "        self.cls = nn.Linear(2 * self.config.hidden_size, 2)\n",
    "\n",
    "        # BERT 모델의 가중치를 초기화.\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        next_sentence_label=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Foward 함수: 두 개의 BERT 모델을 사용하여 입력을 처리한 후, NSP예측 수행.\n",
    "        \"\"\"\n",
    "        outputs = []\n",
    "        # 첫 번째 모델에 입력할 토큰 ID 및 어텐션 마스크 추출\n",
    "        input_ids_1 = input_ids[0]\n",
    "        attention_mask_1 = attention_mask[0]\n",
    "\n",
    "        # 첫 번째 BERT 모델을 통해 hidden state(input_ids) 출력 (pooler output 사용)\n",
    "        outputs.append(self.bert_model_1(input_ids_1, attention_mask=attention_mask_1))\n",
    "\n",
    "        # 두 번째 BERT 모델에 입력할 토큰 ID 및 어텐션 마스크 추출.\n",
    "        input_ids_2 = input_ids[1]\n",
    "        attention_mask_2 = attention_mask[1]\n",
    "\n",
    "        # 두 번째 BERT 모델을 통해 hidden state(input_ids) 출력 (pooler output 사용)\n",
    "        outputs.append(self.bert_model_2(input_ids_2, attention_mask=attention_mask_2))\n",
    "\n",
    "        # 두 BERT 모델의 출력(hidden state)을 연결(concatenation)\n",
    "        # 두 개의 BERT 모델에서 얻은 pooler output을 연결하여 특정 백터 구현\n",
    "        last_hidden_states = torch.cat([output[1] for output in outputs], dim=1)\n",
    "\n",
    "        # 최종적으로 연결된 특징 벡터를 이진 분류 레이어에 전달하여 NSP 예측.\n",
    "        # self.cls 선형함수에 마지막 은닉층 임베딩 상태를 투입하여 로짓 추출.\n",
    "        logits = self.cls(last_hidden_states)\n",
    "\n",
    "        # 크로스엔트로피 손실 (crossentropy loss) 구하기 (손실율 정답과 멀어질 수 록 가중치 부여.)\n",
    "        if next_sentence_label is not None:\n",
    "            # CrossEntropyLoss를 사용하여 NSP 라벨에 대한 손실 계산\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "\n",
    "            next_sentence_label = loss_fct(logits.view(-1, 2), next_sentence_label.view(-1))\n",
    "            return next_sentence_label, logits\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/workspace/colab_try/.venv/lib/python3.10/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 앙상블 트레이닝에 사용할 사전학습 BERT 불러오기\n",
    "# BERT 앙상블 학습 클래스를 인스턴스화하고 이를 GPU에 전달하세요. 아울러\n",
    "# Optimizer 변수에 최적화 함수로 AmdmW를 대입하세요 가중치 감소 가능을 통해 과적합을 방지합니다.\n",
    "import torch\n",
    "from transformers import AdamW\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 모델 및 Config 설정\n",
    "config = BertConfig()\n",
    "model = BertEnsembleForNextSentencePrediction(config)\n",
    "\n",
    "# 토크나이저 설정\n",
    "model.to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "# 학습률 설정\n",
    "learning_rate = 1e-5\n",
    "\n",
    "# 절편과 가중치를 설정.\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "\n",
    "# 최적화 함수 그룹 파라미터 설정\n",
    "optimizer_grouped_parameters = [{\n",
    "    \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "}]\n",
    "# 최적화 함수 설정\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 060 BERT 앙상블 학습 - 데이터 증강\n",
    "def prepare_data(dataset, qa=True):\n",
    "    \"\"\"\n",
    "    BERT 학습을 위한 데이터 전처리 및 증강 함수.\n",
    "\n",
    "    - qa True -> 질문 Q -응답 A 순서로 변환\n",
    "    - qa=False -> 응답A 질분 Q 순서로 데이터 변환 (데이터 증강.)\n",
    "    Args:\n",
    "        dataset (list): [(질문, 답변, 라벨), (질문, 답변, 라벨), ...] 형태의 데이터셋\n",
    "        qa (bool): 순서\n",
    "\n",
    "    Returns:\n",
    "        input_ids(Tensor): 토큰화 된 입력 ID (BERT 모델 입력)\n",
    "        attention_masks(Tensor): 어텐션 마스크 (패딩된 부분을 무시하도록 설정.)\n",
    "        labels(List): 정답 라벨 리스트\n",
    "    \"\"\"    \n",
    "    input_ids, attention_masks = [], []\n",
    "    labels = []\n",
    "\n",
    "    for point in dataset:\n",
    "        if qa is True:\n",
    "            # 데이터셋에서 질문 Q 응답 A 추출.\n",
    "            q, a, _ = point\n",
    "        else:\n",
    "            # 응답A 질문Q 순서를 변경하여 데이터 증강.\n",
    "            a, q, _ = point\n",
    "        # BERT 토크나이저를 사용하여 입력 데이터 변환\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            q,  # 첫 번째 문장 (질문 or 응답)\n",
    "            a,  # 두 번째 문장 (응답. or 질문)\n",
    "            add_special_tokens=True,  # [CLS], [SEP] 토큰 추가\n",
    "            max_length=128,  # 최대 길이 설정\n",
    "            pad_to_max_length=True,  # 길이가 부족한 경우 패딩 추가\n",
    "            return_attention_mask=True,  # 어텐션 마스크 반환 (패딩된 부분 무시)\n",
    "            return_tensors='pt',  # PyTorch 텐서로 반환\n",
    "            truncation=True  # 최대 길이를 초과하는 경우 잘라내기\n",
    "        )\n",
    "        # 변환된 토큰 ID와 어텐션 마스크 저장.\n",
    "        input_ids.append(encoded_dict[\"input_ids\"])\n",
    "        attention_masks.append(encoded_dict[\"attention_mask\"])\n",
    "        # 데이터셋의 마지막 요소(정답 라벨) 저장\n",
    "        labels.append(point[-1])\n",
    "    # 리스트 형태의 텐서들을 하나의 텐서로 병합\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "    return input_ids, attention_masks, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 061 BERT앙상블 학습 커스텀 데이터 세트 정의\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, RandomSampler, Dataset, SequentialSampler\n",
    "\n",
    "# QADatasets 클래스 생성.\n",
    "class QADataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset을 상속받아 BERT 학습을 위한 데이터셋 클래스 정의\n",
    "\n",
    "    Args:\n",
    "        input_ids(Tensor): BERT 입력 토큰 ID(질문 - 응답 쌍의 토큰화 결과)\n",
    "        attention_masks(Tensor): 어텐션 마스크 (패딩된 부분 무시)\n",
    "        labels(Tensor, optional): 정답 라벨 (NSP 학습을 위한 레이블)\n",
    "    \"\"\"\n",
    "    # input_ids 텐서와 attention_masks 텐서 생성\n",
    "    def __init__(self, input_ids, attention_masks, labels=None):\n",
    "        # Numpy 배열로 변환하여 저장 (메모리 최적화)\n",
    "        self.input_ids = np.array(input_ids)\n",
    "        self.attention_mask = np.array(attention_masks)\n",
    "        # 정답 라벨을 Pytorch 텐서 타입으로 변환 \n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.input_ids[index], self.attention_mask[index], self.labels[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.input_ids.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_47482/2364885428.py:10: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  self.input_ids = np.array(input_ids)\n",
      "/tmp/ipykernel_47482/2364885428.py:11: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  self.attention_mask = np.array(attention_masks)\n"
     ]
    }
   ],
   "source": [
    "# BERT 앙상블 학습d을 위한 데이터 로더\n",
    "\n",
    "# prepare_data 함수 호출을 통해 데이터 전처리 수행\n",
    "# 질문 Q 응답 A 순서로 데이터 변환 (기본 설정)\n",
    "input_ids_qa, attention_masks_qa, labels_qa = prepare_data(datasets)\n",
    "\n",
    "# 변환된 데이터를 커스텀 QADataset 클래스에 전달하여 dataset 객체 생성\n",
    "train_dataset_qa = QADataset(input_ids_qa, attention_masks_qa, labels_qa)\n",
    "\n",
    "# 응답 A 질문 Q 순서로 데이터를 변환하여 데이터셋 객체 생성.\n",
    "input_ids_aq, attention_masks_aq, labels_aq = prepare_data(datasets, qa=False)\n",
    "\n",
    "train_dataset_aq = QADataset(input_ids_aq, attention_masks_aq, labels_aq)\n",
    "\n",
    "# 질문 Q -> 응답 A 데이터 로더 \n",
    "dataloader_qa = DataLoader(\n",
    "    dataset=train_dataset_qa,\n",
    "    batch_size=5,\n",
    "    sampler=SequentialSampler(train_dataset_qa)\n",
    ")\n",
    "\n",
    "dataloader_aq = DataLoader(\n",
    "    dataset=train_dataset_aq,\n",
    "    batch_size=5,\n",
    "    sampler=SequentialSampler(train_dataset_aq)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:02<00:00, 11.19it/s, epoch: 29, loss: 0.00048139350838027894]\n"
     ]
    }
   ],
   "source": [
    "# BERT 앙상블 학습 파인튜닝.\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "epochs = 30\n",
    "\n",
    "progress = tqdm(range(epochs))\n",
    "\n",
    "for epoch in progress:\n",
    "    # 0, (qa, aq) \n",
    "    for step, combine_batch in enumerate(zip(dataloader_qa, dataloader_aq)):\n",
    "        # QA AQ 배치를 통해 가져옴 \n",
    "        batch_1, batch_2 = combine_batch\n",
    "        # 모델을 학습모드로 설정.\n",
    "        model.train()\n",
    "        # 데이타를 GPU 또는 CPU로 이동\n",
    "        batch_1 = tuple(t.to(device) for t in batch_1)\n",
    "        batch_2 = tuple(t.to(device) for t in batch_2)\n",
    "\n",
    "        # 모델 입력ㄱ밧 구성 (BERT 앙상블)\n",
    "        inputs = {\n",
    "            \"input_ids\": [batch_1[0], batch_2[0]],\n",
    "            \"attention_mask\": [batch_1[1], batch_2[1]],\n",
    "            \"next_sentence_label\": batch_1[2]\n",
    "        }\n",
    "        # Foward 수행 loss+logits\n",
    "        outputs = model(**inputs)\n",
    "        # 손실 (loss) 값 추출\n",
    "        loss = outputs[0]\n",
    "        # 역전파 수행( 손실값 기준으로 가중치 업데이트)\n",
    "        loss.backward()\n",
    "        # tqdm 진행바에 현재 에포크 및 손실값 출력\n",
    "        progress.set_postfix_str(f\"epoch: {epoch}, loss: {loss}\")\n",
    "        # 옵티마이저 가중치 업데이트\n",
    "        optimizer.step()\n",
    "        # 그래디언트 초가화.\n",
    "        model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_47482/2364885428.py:10: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  self.input_ids = np.array(input_ids)\n",
      "/tmp/ipykernel_47482/2364885428.py:11: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  self.attention_mask = np.array(attention_masks)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.int64(1), np.int64(1), np.int64(0), np.int64(0), np.int64(1)] [np.int64(1), np.int64(1), np.int64(0), np.int64(0), np.int64(1)]\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터 Preprocessing\n",
    "input_ids_qa, attention_masks_qa, labels_qa = prepare_data(datasets)\n",
    "\n",
    "test_dataset_qa = QADataset(input_ids_qa, attention_masks_qa, labels_qa)\n",
    "\n",
    "input_ids_aq, attention_masks_aq, labels_aq = prepare_data(datasets, qa=False)\n",
    "\n",
    "test_dataset_aq = QADataset(input_ids_aq, attention_masks_aq, labels_aq)\n",
    "\n",
    "dataloader_qa = DataLoader(\n",
    "    dataset=test_dataset_qa,\n",
    "    batch_size=16,\n",
    "    sampler=SequentialSampler(test_dataset_qa)\n",
    ")\n",
    "\n",
    "\n",
    "dataloader_aq = DataLoader(\n",
    "    dataset=test_dataset_aq,\n",
    "    batch_size=16,\n",
    "    sampler=SequentialSampler(test_dataset_aq)\n",
    ")\n",
    "\n",
    "complete_outputs, complete_label_ids = [], []\n",
    "\n",
    "for step, combine_batch in enumerate(zip(dataloader_qa, dataloader_aq)):\n",
    "    model.eval()\n",
    "\n",
    "    batch_1, batch_2 = combine_batch\n",
    "\n",
    "    batch_1 = tuple(t.to(device) for t in batch_1)\n",
    "    batch_2 = tuple(t.to(device) for t in batch_2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inputs = {\n",
    "            \"input_ids\": [batch_1[0], batch_2[0]],\n",
    "            \"attention_mask\": [batch_1[1], batch_2[1]],\n",
    "            \"next_sentence_label\": batch_1[2]\n",
    "        }\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        tmp_eval_loss, logits = outputs[:2]\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "\n",
    "        outputs = np.argmax(logits, axis=1)\n",
    "\n",
    "        labels_ids = inputs['next_sentence_label'].detach().cpu().numpy()\n",
    "\n",
    "    complete_outputs.extend(outputs)\n",
    "    complete_label_ids.extend(labels_ids)\n",
    "    \n",
    "print(complete_outputs, complete_label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/workspace/colab_try/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2690: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_47482/2364885428.py:10: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  self.input_ids = np.array(input_ids)\n",
      "/tmp/ipykernel_47482/2364885428.py:11: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  self.attention_mask = np.array(attention_masks)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.int64(1)] [np.int64(1)]\n"
     ]
    }
   ],
   "source": [
    "datasets = [[\"what music do you like?\", \"I like Rock Music\", 1]]\n",
    "\n",
    "input_ids_qa, attention_masks_qa, labels_qa = prepare_data(datasets)\n",
    "\n",
    "test_dataset_qa = QADataset(input_ids_qa, attention_masks_qa, labels_qa)\n",
    "\n",
    "input_ids_aq, attention_masks_aq, labels_aq = prepare_data(datasets, qa=False)\n",
    "\n",
    "test_dataset_aq = QADataset(input_ids_aq, attention_masks_aq, labels_aq)\n",
    "\n",
    "dataloader_qa = DataLoader(\n",
    "    dataset=test_dataset_qa,\n",
    "    batch_size=16,\n",
    "    sampler=SequentialSampler(test_dataset_qa)\n",
    ")\n",
    "\n",
    "\n",
    "dataloader_aq = DataLoader(\n",
    "    dataset=test_dataset_aq,\n",
    "    batch_size=16,\n",
    "    sampler=SequentialSampler(test_dataset_aq)\n",
    ")\n",
    "\n",
    "complete_outputs, complete_label_ids = [], []\n",
    "\n",
    "for step, combine_batch in enumerate(zip(dataloader_qa, dataloader_aq)):\n",
    "    model.eval()\n",
    "\n",
    "    batch_1, batch_2 = combine_batch\n",
    "\n",
    "    batch_1 = tuple(t.to(device) for t in batch_1)\n",
    "    batch_2 = tuple(t.to(device) for t in batch_2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inputs = {\n",
    "            \"input_ids\": [batch_1[0], batch_2[0]],\n",
    "            \"attention_mask\": [batch_1[1], batch_2[1]],\n",
    "            \"next_sentence_label\": batch_1[2]\n",
    "        }\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        tmp_eval_loss, logits = outputs[:2]\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "\n",
    "        outputs = np.argmax(logits, axis=1)\n",
    "\n",
    "        labels_ids = inputs['next_sentence_label'].detach().cpu().numpy()\n",
    "\n",
    "    complete_outputs.extend(outputs)\n",
    "    complete_label_ids.extend(labels_ids)\n",
    "\n",
    "print(complete_outputs, complete_label_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
