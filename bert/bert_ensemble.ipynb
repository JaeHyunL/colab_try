{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "최근 자연어 처리 커뮤니티에서는 트랜스포머 기반의 여러 사전 학습모델을 묶어 만든 앙상블(ensemble) 모델이\n",
    "\n",
    "뛰어난 성능을 보여준다.\n",
    "\n",
    "질의응답 직업 벤치마크 결과를 섞어 만든 SQuAD2.0에 최고 성능이 모델 10개가 모두 앙상블 모델일 정도입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [ \n",
    "    ['What music do you like?', 'I like Rock music.', 1],\n",
    "    ['What is your favorite food?', 'I like sushi the best', 1],\n",
    "    ['What is your favorite color?', \"I'm going to be a doctor\", 0],\n",
    "    ['What is your favorite song?', \"Tokyo olympic game in 2020 was postponed\", 0],\n",
    "    ['Do you like watching TV shows?', \"Yeah, I often watch it in my spear time\", 1]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT 앙상블 클래스 정의\n",
    "# NSP 작업을 위해 두 개의 BERT를 병행 연결하고 마지막 층은 선형 결합층이 되도록 \n",
    "# 앙상블 학습 클래스를 정의하시오. BertPreTrainedModel 클래스로부터 상속을 받아\n",
    "# 사전학습(pre-training)과 재학습(re-training)이 가능합니다. 클래스 개념은 문제 002를 상속 개념은 008문제를 참조하시오.\n",
    "\n",
    "from transformers import BertPreTrainedModel, BertConfig, BertModel, BertTokenizer, AdamW\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "# 클래스 정의\n",
    "class BertEnsembleForNextSentencePrediction(BertPreTrainedModel):\n",
    "\n",
    "    # 생성자 설정\n",
    "    def __init__(self, config, *args, **kwargs):\n",
    "        super().__init__(config)\n",
    "\n",
    "        # QA BERT 모델\n",
    "        self.bert_model_1 = BertModel(config)\n",
    "        # AQ BERT 모델\n",
    "        self.bert_model_2 = BertModel(config)\n",
    "\n",
    "        # Linear function\n",
    "        self.cls = nn.Linear(2 * self.config.hidden_size, 2)\n",
    "\n",
    "        # initial weight\n",
    "        self.init_weights()\n",
    "\n",
    "    # forward 신경망 설정\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        next_sentence_label=None\n",
    "    ):\n",
    "        outputs = []\n",
    "        \n",
    "        # 첫 번째 입력 문자 저장\n",
    "        input_ids_1 = input_ids[0]\n",
    "\n",
    "        # input_ids 첫 번째 입력 문장의 attention mask \n",
    "        attention_mask_1 = attention_mask[0]\n",
    "\n",
    "        # Bert_model_1에 input_ids1 투입한 결과를 outputs에 순차적으로 저장\n",
    "        outputs.append(self.bert_model_1(input_ids_1, attention_mask=attention_mask_1))\n",
    "\n",
    "        # input_ids 두 번째 입력(문장) 저장\n",
    "        input_ids_2 = input_ids[1]\n",
    "\n",
    "        # inputs 두 번째 입력 문장의 attention_mask 저장\n",
    "        attention_mask_2 = attention_mask[1]\n",
    "\n",
    "        # bert_model_2에 input_ids2 투입한 결과를 outputs에 순차적으로 저장\n",
    "        outputs.append(self.bert_model_2(input_ids_2, attention_mask=attention_mask_2))\n",
    "\n",
    "        # outputs에 쌓인 otuput의 두 번째 요소(output[1])를 하나씩 추출하여\n",
    "        # torch.cat()으로 토치 텐서 형태로 병합\n",
    "        # 이를 통해 마지막 은닉층 임베딩 상태를 구함.\n",
    "        last_hidden_states = torch.cat([output[1] for output in outputs], dim=1)\n",
    "\n",
    "        # self.cls 선형함수에 마지막 은닉층 임베딩 상태를 투입하여 로짓 추출\n",
    "        logits = self.cls(last_hidden_states)\n",
    "\n",
    "        # 크로스 엔트로피 손실(crossentropyloss) 구하기\n",
    "        if next_sentence_label is not None:\n",
    "            # nn.CrossEntropyLoss( ) 입력 데이터의 마지막 인덱스는 계산에서 제외\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "            # logits.view(-1, 2)는 열이 두 개 형태로 logits를 정렬\n",
    "            # next_sentence_label.view(-1)는 행이 하나인 형태로 정렬\n",
    "            next_sentence_label = loss_fct(logits.view(-1, 2), next_sentence_label.view(-1))\n",
    "            return next_sentence_label, logits\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/workspace/colab_try/.venv/lib/python3.10/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 앙상블 트레이닝에 사용할 사전학습 BERT 불러오기\n",
    "# BERT 앙상블 학습 클래스를 인스턴스화하고 이를 GPU에 전달하세요. 아울러\n",
    "# Optimizer 변수에 최적화 함수로 AmdmW를 대입하세요 가중치 감소 가능을 통해 과적합을 방지합니다.\n",
    "import torch\n",
    "from transformers import AdamW\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 모델 및 Config 설정\n",
    "config = BertConfig()\n",
    "model = BertEnsembleForNextSentencePrediction(config)\n",
    "\n",
    "# 토크나이저 설정\n",
    "model.to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "# 학습률 설정\n",
    "learning_rate = 1e-5\n",
    "\n",
    "# 절편과 가중치를 설정.\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "\n",
    "# 최적화 함수 그룹 파라미터 설정\n",
    "optimizer_grouped_parameters = [{\n",
    "    \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "}]\n",
    "# 최적화 함수 설정\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 060 BERT 앙상블 학습 - 데이터 증강\n",
    "def prepare_data(dataset, qa=True):\n",
    "    input_ids, attention_masks = [], []\n",
    "    labels = []\n",
    "\n",
    "    for point in dataset:\n",
    "        if qa is True:\n",
    "            # point에 있는 3개의 원소를 앞에 요소부터 q, a, _으로 배경\n",
    "            q, a, _ = point\n",
    "        else:\n",
    "            a, q, _ = point\n",
    "\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            q,\n",
    "            a,\n",
    "            add_special_tokens=True,\n",
    "            max_length=128,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "        input_ids.append(encoded_dict[\"input_ids\"])\n",
    "\n",
    "        attention_masks.append(encoded_dict[\"attention_mask\"])\n",
    "\n",
    "        labels.append(point[-1])\n",
    "\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    \n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "    return input_ids, attention_masks, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 061 BERT앙상블 학습 커스텀 데이터 세트 정의\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, RandomSampler, Dataset, SequentialSampler\n",
    "\n",
    "# QADatasets 클래스 생성.\n",
    "class QADataset(Dataset):\n",
    "\n",
    "    # input_ids 텐서와 attention_masks 텐서 생성\n",
    "    def __init__(self, input_ids, attention_masks, labels=None):\n",
    "        self.input_ids = np.array(input_ids)\n",
    "        self.attention_mask = np.array(attention_masks)\n",
    "        # torch.long은 정수 (integer)타입을 의미\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.input_ids[index], self.attention_mask[index], self.labels[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.input_ids.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_47482/2364885428.py:10: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  self.input_ids = np.array(input_ids)\n",
      "/tmp/ipykernel_47482/2364885428.py:11: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  self.attention_mask = np.array(attention_masks)\n"
     ]
    }
   ],
   "source": [
    "# BERT 앙상블 학습 데이타 로더\n",
    "input_ids_qa, attention_masks_qa, labels_qa = prepare_data(datasets)\n",
    "\n",
    "train_dataset_qa = QADataset(input_ids_qa, attention_masks_qa, labels_qa)\n",
    "\n",
    "input_ids_aq, attention_masks_aq, labels_aq = prepare_data(datasets, qa=False)\n",
    "\n",
    "train_dataset_aq = QADataset(input_ids_aq, attention_masks_aq, labels_aq)\n",
    "\n",
    "dataloader_qa = DataLoader(\n",
    "    dataset=train_dataset_qa,\n",
    "    batch_size=5,\n",
    "    sampler=SequentialSampler(train_dataset_qa)\n",
    ")\n",
    "\n",
    "dataloader_aq = DataLoader(\n",
    "    dataset=train_dataset_aq,\n",
    "    batch_size=5,\n",
    "    sampler=SequentialSampler(train_dataset_aq)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:02<00:00, 11.19it/s, epoch: 29, loss: 0.00048139350838027894]\n"
     ]
    }
   ],
   "source": [
    "# BERT 앙상블 학습 파인튜닝.\n",
    "from tqdm import tqdm\n",
    "epochs = 30\n",
    "progress = tqdm(range(epochs))\n",
    "for epoch in progress:\n",
    "\n",
    "    for step, combine_batch in enumerate(zip(dataloader_qa, dataloader_aq)):\n",
    "        batch_1, batch_2 = combine_batch\n",
    "        # 모델 \n",
    "        model.train()\n",
    "\n",
    "        batch_1 = tuple(t.to(device) for t in batch_1)\n",
    "        batch_2 = tuple(t.to(device) for t in batch_2)\n",
    "\n",
    "        inputs = {\n",
    "            \"input_ids\": [batch_1[0], batch_2[0]],\n",
    "            \"attention_mask\": [batch_1[1], batch_2[1]],\n",
    "            \"next_sentence_label\": batch_1[2]\n",
    "        }\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        loss = outputs[0]\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # print(f\"epoch: {epoch}, loss: {loss}\")\n",
    "        progress.set_postfix_str(f\"epoch: {epoch}, loss: {loss}\")\n",
    "        optimizer.step()\n",
    "\n",
    "        model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_47482/2364885428.py:10: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  self.input_ids = np.array(input_ids)\n",
      "/tmp/ipykernel_47482/2364885428.py:11: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  self.attention_mask = np.array(attention_masks)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.int64(1), np.int64(1), np.int64(0), np.int64(0), np.int64(1)] [np.int64(1), np.int64(1), np.int64(0), np.int64(0), np.int64(1)]\n"
     ]
    }
   ],
   "source": [
    "input_ids_qa, attention_masks_qa, labels_qa = prepare_data(datasets)\n",
    "\n",
    "test_dataset_qa = QADataset(input_ids_qa, attention_masks_qa, labels_qa)\n",
    "\n",
    "input_ids_aq, attention_masks_aq, labels_aq = prepare_data(datasets, qa=False)\n",
    "\n",
    "test_dataset_aq = QADataset(input_ids_aq, attention_masks_aq, labels_aq)\n",
    "\n",
    "dataloader_qa = DataLoader(\n",
    "    dataset=test_dataset_qa,\n",
    "    batch_size=16,\n",
    "    sampler=SequentialSampler(test_dataset_qa)\n",
    ")\n",
    "\n",
    "\n",
    "dataloader_aq = DataLoader(\n",
    "    dataset=test_dataset_aq,\n",
    "    batch_size=16,\n",
    "    sampler=SequentialSampler(test_dataset_aq)\n",
    ")\n",
    "\n",
    "complete_outputs, complete_label_ids = [], []\n",
    "\n",
    "for step, combine_batch in enumerate(zip(dataloader_qa, dataloader_aq)):\n",
    "    model.eval()\n",
    "\n",
    "    batch_1, batch_2 = combine_batch\n",
    "\n",
    "    batch_1 = tuple(t.to(device) for t in batch_1)\n",
    "    batch_2 = tuple(t.to(device) for t in batch_2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inputs = {\n",
    "            \"input_ids\": [batch_1[0], batch_2[0]],\n",
    "            \"attention_mask\": [batch_1[1], batch_2[1]],\n",
    "            \"next_sentence_label\": batch_1[2]\n",
    "        }\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        tmp_eval_loss, logits = outputs[:2]\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "\n",
    "        outputs = np.argmax(logits, axis=1)\n",
    "\n",
    "        labels_ids = inputs['next_sentence_label'].detach().cpu().numpy()\n",
    "\n",
    "    complete_outputs.extend(outputs)\n",
    "    complete_label_ids.extend(labels_ids)\n",
    "    \n",
    "print(complete_outputs, complete_label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/workspace/colab_try/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2690: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_47482/2364885428.py:10: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  self.input_ids = np.array(input_ids)\n",
      "/tmp/ipykernel_47482/2364885428.py:11: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  self.attention_mask = np.array(attention_masks)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.int64(1)] [np.int64(1)]\n"
     ]
    }
   ],
   "source": [
    "datasets = [[\"what music do you like?\", \"I like Rock Music\", 1]]\n",
    "\n",
    "input_ids_qa, attention_masks_qa, labels_qa = prepare_data(datasets)\n",
    "\n",
    "test_dataset_qa = QADataset(input_ids_qa, attention_masks_qa, labels_qa)\n",
    "\n",
    "input_ids_aq, attention_masks_aq, labels_aq = prepare_data(datasets, qa=False)\n",
    "\n",
    "test_dataset_aq = QADataset(input_ids_aq, attention_masks_aq, labels_aq)\n",
    "\n",
    "dataloader_qa = DataLoader(\n",
    "    dataset=test_dataset_qa,\n",
    "    batch_size=16,\n",
    "    sampler=SequentialSampler(test_dataset_qa)\n",
    ")\n",
    "\n",
    "\n",
    "dataloader_aq = DataLoader(\n",
    "    dataset=test_dataset_aq,\n",
    "    batch_size=16,\n",
    "    sampler=SequentialSampler(test_dataset_aq)\n",
    ")\n",
    "\n",
    "complete_outputs, complete_label_ids = [], []\n",
    "\n",
    "for step, combine_batch in enumerate(zip(dataloader_qa, dataloader_aq)):\n",
    "    model.eval()\n",
    "\n",
    "    batch_1, batch_2 = combine_batch\n",
    "\n",
    "    batch_1 = tuple(t.to(device) for t in batch_1)\n",
    "    batch_2 = tuple(t.to(device) for t in batch_2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inputs = {\n",
    "            \"input_ids\": [batch_1[0], batch_2[0]],\n",
    "            \"attention_mask\": [batch_1[1], batch_2[1]],\n",
    "            \"next_sentence_label\": batch_1[2]\n",
    "        }\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        tmp_eval_loss, logits = outputs[:2]\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "\n",
    "        outputs = np.argmax(logits, axis=1)\n",
    "\n",
    "        labels_ids = inputs['next_sentence_label'].detach().cpu().numpy()\n",
    "\n",
    "    complete_outputs.extend(outputs)\n",
    "    complete_label_ids.extend(labels_ids)\n",
    "\n",
    "print(complete_outputs, complete_label_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
