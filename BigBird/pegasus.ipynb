{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PegasusForConditionalGeneration(\n",
       "  (model): PegasusModel(\n",
       "    (shared): Embedding(96103, 1024, padding_idx=0)\n",
       "    (encoder): PegasusEncoder(\n",
       "      (embed_tokens): Embedding(96103, 1024, padding_idx=0)\n",
       "      (embed_positions): PegasusSinusoidalPositionalEmbedding(512, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-15): 16 x PegasusEncoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): ReLU()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): PegasusDecoder(\n",
       "      (embed_tokens): Embedding(96103, 1024, padding_idx=0)\n",
       "      (embed_positions): PegasusSinusoidalPositionalEmbedding(512, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-15): 16 x PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=96103, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 페가수스 모델은 트랜스포머를 사용하고 문장 요약에 특화된 사정전학습 모델임.\n",
    "# PEGASUS 라이브러리 설정 및 사전학습 모델 불러오기.\n",
    "\n",
    "# 구글 코랩에서 PEGASUS의 자동문장 요약을 실행하기 위해 필요한 모듈을 설치합니다.\n",
    "# 그리고 사전학습 모델인 google/pegasus-xsum을 불러옵니다.\n",
    "import torch\n",
    "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = PegasusForConditionalGeneration.from_pretrained('google/pegasus-xsum').to(device)\n",
    "tokenizer = PegasusTokenizer.from_pretrained('google/pegasus-xsum')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  959, 53315, 29622, 49727,   143, 53046,   151,   110,   105,   206,\n",
       "          9976,  6186,   151,   110,   105,   206,  8514,   107, 48416, 56332,\n",
       "          6827,   312,   163,   606,   130,   959, 53315, 29622,  6827,   132,\n",
       "           959, 53315, 29622, 49727,  6827,   108,   140,   109,   674,  7940,\n",
       "         13915,   113,   109,  7845,   661, 28151,   107,  9973,   115,   305,\n",
       "         48437,   108,   126,   117,   746,   115,  4905, 18292,   108,   793,\n",
       "          4611,   107,   139,  1368,   113,   109,  6418,  2913,  6827,   116,\n",
       "           836,   141,   109,  7845,   661, 28151,   108,   959, 53315, 29622,\n",
       "         49727,  1502,   130,   109,   238,   113,   109,  7940,   191, 42364,\n",
       "          7434,   328,   111,   109,  2236,   113,   657,   107,  1064, 53315,\n",
       "         29622, 49727,  2059,   112,  1278,   130,   109,   674, 13915,   113,\n",
       "           109,  7845,   661, 28151,   430,   109,  6751,   195,  6487,   141,\n",
       "          1316,   333,   109,   125,   208, 22390,  1981, 18751, 14762, 70952,\n",
       "         66780,   111,  7771,   118,   228,  6468,   107,   611,   108,   115,\n",
       "           109,  1925,   307,  1902,   108,   149,   113,   109, 13915,   131,\n",
       "           116,   624, 23285,  1637,   195,  7438,   365,   109,  2071,   113,\n",
       "          5776, 34527,   285, 39025, 19028,   333,   109, 14729,   113,  2048,\n",
       "          1859, 64413,   107,  1027,  2527,  2511,   195,  7438,   124,   114,\n",
       "           378,   113,   204,  1466, 20369,   139,  6043,  3733,   113,  3266,\n",
       "          4611,   195,  6569,   190,   109,  3636,   111,  2436,   113,   109,\n",
       "          7845,   661,  7940,  1462,   107,   315,   109,   616,   599,   307,\n",
       "          1902,   108,   249,   113,   109, 13915,   140, 18873,  6487,   141,\n",
       "         12513,  2466,   333,   203,  9807,   113,  4611,   651,  1268,  7781,\n",
       "         42463,   126,   140,  5957,   130,   114,  2345,   593,  1685,   109,\n",
       "          5667,   116,   108,   109, 31387, 13915,  1482,   117,  6861,   270,\n",
       "          7438,   112,   203,   856,   515,   107,   168,   163,  2671,   109,\n",
       "           765,  6827,  2447,   111,   109,   765, 18619,  2447,   373,   109,\n",
       "          6751,   113,   109,  1482,   107, 50538,  1064, 53315, 29622, 49727,\n",
       "           108,   746,   115,  2523,   113,   959, 27867, 31585, 21418,  4195,\n",
       "           108,   140,   836,   339,   231,   244,   109,  7845,   661, 28151,\n",
       "           140,  3271,   111,   126,  1502,   130,   203,   674, 13915,   441,\n",
       "           109,  2924, 14741,  6189, 10951,   893,   126,   111,   109,  1411,\n",
       "           113,  7651, 21754,   143, 28456,   131,   116,  8653, 64413,  1804,\n",
       "           158,   833,   959, 27867, 31585, 21418,  9383,   108,   109,   674,\n",
       "          3742,   112,   109, 13915,   108,   959, 53315, 29622, 49727,   140,\n",
       "          4612,   115,   109,   773,   113,   109,  5829,  1863,   517,   107,\n",
       "           168,   140, 12438,  4617,   269,   270,  2785,   112, 21763,   333,\n",
       "           109,  2769, 13514,   113, 34674,  1979,   581,   109,   352, 58980,\n",
       "           231,   109, 13915,  5328,   195,   518, 50761,   430,   270, 16533,\n",
       "           115, 43838,   365,   109,  2071,   113, 34527,   285, 39025, 19028,\n",
       "         54041, 68851,  3036,   107,   139,  5052,   140,  1413,   124,   114,\n",
       "          4168,  2116,   108,   122, 20682,  2511,  9702,   424,   115,   114,\n",
       "         41632,  4191,  4460,   107,  5687,   109, 13915,  2622,   195,   109,\n",
       "         26266,  1975,   143, 18255,  9208,   661,   312,  3300,   118,   109,\n",
       "          4138,   111,   449,  2662,   108,   111,   109, 17362,  1975,   143,\n",
       "         54784,  9208,   661,   312,   162,   953,   622, 10965,   118,   109,\n",
       "          7940,   328,   130,   210,   130,  4938,   118,  7485,   107,  5687,\n",
       "           203,  2248, 59298,   195,   176, 35017,   108,   423,   111,   360,\n",
       "           108,   330, 23553, 49727,   143,   544,  3991,   131,   116,  4075,\n",
       "           158,   111, 21156, 49727,   143,   544,  8715,  5776,   131,   116,\n",
       "          4075,   250,  9701,   112,   203,  1932,   130,   109,  5087,   113,\n",
       "          1146,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토크나이징 및 인코딩\n",
    "# 영어 문장을 준비해서    리스트 타입변수 inputs에 넣으시오 \n",
    "import re\n",
    "english_sentence = \"\"\"\n",
    "Gyeongbokgung (Korean: 경복궁; Hanja: 景福宮; lit. Radiant Prosperity Palace), also known as Gyeongbok Palace or Gyeongbokgung Palace, was the main royal palace of the Joseon dynasty. Built in 1395, it is located in northern Seoul, South Korea. The largest of the Five Grand Palaces built by the Joseon dynasty, Gyeongbokgung served as the home of the royal/imperial family and the seat of government.\n",
    "\n",
    "Gyeongbokgung continued to serve as the main palace of the Joseon dynasty until the premises were destroyed by fire during the Imjin War (1592–1598) and abandoned for two centuries. However, in the 19th century, all of the palace's 7,700 rooms were restored under the leadership of Prince Regent Heungseon during the reign of King Gojong. Some 500 buildings were restored on a site of over 40 hectares.[1][2] The architectural principles of ancient Korea were incorporated into the tradition and appearance of the Joseon royal court.\n",
    "\n",
    "In the early 20th century, much of the palace was systematically destroyed by Imperial Japan during its occupation of Korea.[3] On January 21, 1963, it was designated as a cultural property.[4] Since the 1990s, the walled palace complex is gradually being restored to its original form. It also houses the National Palace Museum and the National Folk Museum within the premises of the complex.\n",
    "\n",
    "Overview\n",
    "Gyeongbokgung, located in north of Gwanghwamun Square, was built three years after the Joseon dynasty was founded and it served as its main palace.[5] With the mountain Bugaksan behind it and the Street of Six Ministries (today's Sejongno) outside Gwanghwamun Gate, the main entrance to the palace, Gyeongbokgung was situated in the heart of the Korean capital city. It was steadily expanded before being reduced to ashes during the Japanese invasion of 1592.\n",
    "\n",
    "For the next 273 years the palace grounds were left derelict until being rebuilt in 1867 under the leadership of Regent Heungseon Daewongun. The restoration was completed on a grand scale, with 330 buildings crowded together in a labyrinthine configuration. Within the palace walls were the Outer Court (oejeon), offices for the king and state officials, and the Inner Court (naejeon), which included living quarters for the royal family as well as gardens for leisure. Within its extensive precincts were other palaces, large and small, including Junggung (the Queen's residence) and Donggung (the Crown Prince's residence).\n",
    "\n",
    "Due to its status as the symbol of national sovereignty, Gyeongbokgung was extensively damaged during the Japanese occupation of the early 20th century. In 1911, ownership of land at the palace was transferred to the Japanese governor-general. In 1915, on the pretext of holding an exhibition, more than 90% of the buildings were torn down. Following the exhibition, the Japanese leveled whatever still remained and built their colonial headquarters, the Government-General Building (1916–26), on the site. Only a handful of iconic structures survived, including the Throne Hall and Gyeonghoeru Pavilion.\n",
    "\n",
    "Restoration efforts have been ongoing since 1990. The Government-General Building was removed in 1996 and Heungnyemun Gate (2001) and Gwanghwamun Gate (2006–2010) were reconstructed in their original locations and forms. Reconstructions of the Inner Court and Crown Prince's residence have also been completed.\n",
    "\n",
    "The current total area is 415,800 square metres (4,476,000 sq ft).[6]\n",
    "\n",
    "History\n",
    "14th–16th centuries\n",
    "\n",
    "King Taejo\n",
    "Gyeongbokgung was originally constructed in 1394 by King Taejo, the first king and the founder of the Joseon dynasty, and its name was conceived by an influential government minister named Jeong Do-jeon. Afterwards, the palace was continuously expanded during the reign of King Taejong and King Sejong the Great. It was severely damaged by fire in 1553, and its costly restoration, ordered by King Myeongjong, was completed in the following year.\n",
    "\n",
    "However, four decades later, Gyeongbokgung was burnt to the ground during the Japanese invasions of Korea of 1592–1598 when Koreans angry at the court of King Seonjo for evacuating Seoul torched the royal residence.[7] The royal court was moved to the palace Changdeokgung. Gyeongbokgung site was left in ruins for the next three centuries.[8]\n",
    "\n",
    "19th century\n",
    "In 1867, during the regency of Daewongun, the palace buildings were reconstructed and formed a massive complex with 330 buildings and 5,792 rooms. Standing on 4,657,576 square feet (432,703 square meters) of land, Gyeongbokgung again became an iconic symbol for both the Korean nation and the Korean royal family. In 1894, the Japanese occupied the palace and forced Gojong to establish a pro-Japanese government. In 1895, after the assassination of Empress Myeongseong by Japanese agents, her husband, Emperor Gojong, left the palace. The Imperial Family never returned to Gyeongbokgung.[9]\n",
    "\n",
    "\"\"\"\n",
    "english_sentence = re.sub(r\"[:.]\\[[0-9]+\\]\\([0-9]+\\)|.?[([][0-9]+[])]|\\n|\\r\", r\"\", english_sentence)\n",
    "input = tokenizer(english_sentence, truncation=True, padding='longest', return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'method' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      2\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      4\u001b[0m     result \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(result)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'method' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    result = model(**input)\n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
