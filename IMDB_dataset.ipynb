{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall torchtext\n",
    "!pip install torchtext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall portalocker\n",
    "!pip install portalocker --no-cache-dir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall accelerate\n",
    "!pip install accelerate -U --no-cache-dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/workspace/colab_try/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using the latest cached version of the dataset since imdb couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'plain_text' at /home/oem/.cache/huggingface/datasets/imdb/plain_text/0.0.0/e6281661ce1c48d982bc483cf8a173c1bbeb5d31 (last modified on Tue Feb 18 11:09:06 2025).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.', 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# IMDB 데이터셋 로드\n",
    "imdb_dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# 훈련 및 테스트 데이터 분리\n",
    "train_iter = imdb_dataset[\"train\"]\n",
    "test_iter = imdb_dataset[\"test\"]\n",
    "\n",
    "# 데이터 샘플 확인\n",
    "print(train_iter[0])  # 첫 번째 리뷰 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCC\n",
      "{'text': 'I thought that the nadir of horror film making had been reached with \"Book of Shadows\", I was wrong. This film makes that look like \"The Magnficiant Ambersons\" compared to this piece of shameless, unexpurgated fecal matter that has the audacity to call itself a movie. I\\'d write more but I\\'m still to angry that I was idiot enough to spend £3 renting it, bobbins.<br /><br />And were these people English? and where is the forest> I have lived in the UK two thirds of my life and as far as I know there are no dark uncharted woodlands in the midlands. The whole bally thing looked like a national trust conifer plantation. Those angels looked like anorexic pornstars (turned most of them were, did my research). I did however like the bit when Judd got ripped in pieces.<br /><br />P.S I love and admire Tom Savini but HE CANNOT ACT', 'label': 0}\n",
      "{'text': \"Seven months since a revelatory viewing of Faces, I finally found a rentable DVD copy of Cassavetes' first feature. Shot on a shoestring in Manhattan and in his acting workshop on ad hoc sets, Shadows was the culmination of months of improvisational rehearsals, in which the (mostly amateur) actors developed bonds with one another, invented their characters, and polished their techniques to give their filmed performances just the right tenor of spontaneous familiarity. This intimate approach led to some incredibly daring work in Faces\\x97i.e., Seymour Cassel cramming his hands down Lynn Carlin's throat in an attempt to revive her from an overdose\\x97just as the actors' utter conviction here yields blisteringly honest moments like Lelia and Tony's post-coital assessment of their relationship and Ben's revulsion at a black woman's touch as a manifestation of his racial confusion and self-loathing. A homemade production in the best sense, the out-of-sync dubbing and sound recording, and the granular cinematography and up-close camera setups, build an immersive atmosphere that perfectly suits Cassavetes' nuanced vision of human relationships as perpetual works in progress, marked by desperate emotional fluctuations and wistful attempts at communication and understanding. Charles Mingus's largely improvised jazz score is an ideal complement to the film's vision of living by the moment, a mantra by which Cassavetes worked and seemingly lived.\", 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# IMDB 데이터셋 로드\n",
    "imdb_dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# 훈련 및 테스트 데이터 분리\n",
    "train_iter = imdb_dataset[\"train\"]\n",
    "test_iter = imdb_dataset[\"test\"]\n",
    "\n",
    "\n",
    "import random\n",
    "random.seed(7)\n",
    "\n",
    "train_list = list(train_iter)\n",
    "test_list = list(test_iter)\n",
    "train_lists_small = random.sample(train_list, 1000)\n",
    "test_lists_small = random.sample(test_list, 1000)\n",
    "print(\"CCC\")\n",
    "\n",
    "print(train_lists_small[0])\n",
    "print(test_lists_small[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 레이블 인코딩\n",
    "\n",
    "- 텍스트와 레이블을 Tuple pair\n",
    "- 긍정 2\n",
    "- 부정 1\n",
    "- 2를 -> 1로\n",
    "- 1을 -> 2로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0\n",
      "label\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "train_texts = []\n",
    "train_labels = []\n",
    "\n",
    "# train_lists_small에 담긴 튜플 쌍 원소를 변수명 label과 text를 부여하여 순서대로 추출\n",
    "\n",
    "for label, text in train_lists_small:\n",
    "    # IMDB 데이터의 기존 레이블 2를 1로 변경 기존 레이블 1을 0으로 변경\n",
    "    train_labels.append(1 if label==2 else 0)\n",
    "    train_texts.append(text)\n",
    "\n",
    "test_texts = []\n",
    "test_labels = []\n",
    "\n",
    "for label, text in test_lists_small:\n",
    "    test_labels.append(1 if label==2 else 0)\n",
    "    test_texts.append(text)\n",
    "\n",
    "print(train_texts[0])\n",
    "print(train_labels[0])\n",
    "print(test_texts[0])\n",
    "print(test_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 및 검증 데이터 분리\n",
    "학습데이터 800개와 테스트 검증데이터 200개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "800\n",
      "200\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_texts, val_texts, train_labels,  val_labels = train_test_split(train_texts, train_labels, test_size=.2, random_state=2)\n",
    "print(len(train_texts))\n",
    "print(len(train_labels))\n",
    "print(len(val_texts))\n",
    "print(len(val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토크나이징 및 인코딩\n",
    "사전 학습된 distilbert-based-uncased 모델에 투입하기 위해 토크나이저를 사용하여 인코딩\n",
    "\n",
    "토큰은 입력 텍스트의 기본 구성요소를 의미하며, 일반적으로 단어를 지칭합니다.\n",
    "토크나이징은 입력 텍스트를 단어로 구분해 내는 과정입니다.\n",
    "\n",
    "토크나이징 결과를 딥러닝 모델이 사용하기 위해서는 잘게 구분된 토큰(텍스트 형태)을 숫자로 전환해야 합니다.\n",
    "이 전환과정을 인코딩이라고 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 3830, 102]\n",
      "[CLS] label [SEP]\n"
     ]
    }
   ],
   "source": [
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
    "\n",
    "print(train_encodings[\"input_ids\"][0][:5])\n",
    "print(tokenizer.decode(train_encodings[\"input_ids\"][0][:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문제\n",
    "\n",
    "데이터세트 클래스 생성\n",
    "\n",
    "문제: Torch.utils.data.Dataset을 상속하는 IMDbDataset 라는 클래스를 스스로 작성하세요.\n",
    "\n",
    "그리고 문제 007의 IMDb 데이터세트에서 생성한 학습 (Train_encodings), 검증(val_encodings), 테스트 데이터세트(test_encodings)를 입력해서\n",
    "\n",
    "이 클래스를 인스턴스화합니다. 클래스 및 인스턴스화 개념은 문제 002를 참고하세요. 이를 데이터세트 후속 문제에서 파인튜닝에 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([ 101, 3830,  102]), 'attention_mask': tensor([1, 1, 1]), 'labels': tensor(0)}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "class IMDbDataset(DistilBertTokenizerFast):\n",
    "    \n",
    "    def __init__(self, encodings, labels):\n",
    "        \"\"\"생성자\n",
    "\n",
    "        Args:\n",
    "            encodings (_type_): 초기화\n",
    "            labels (_type_): 초기화\n",
    "        \"\"\"        \n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        \"\"\"아이템 반환 매서드\n",
    "\n",
    "        Args:\n",
    "            idx (integer): index\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"        \n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = IMDbDataset(train_encodings, train_labels)\n",
    "val_dataset = IMDbDataset(val_encodings, val_labels)\n",
    "test_dataset = IMDbDataset(test_encodings, test_labels)\n",
    "\n",
    "# 반복 가능 타입, 딕셔너리\n",
    "for i in train_dataset:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사전 학습 모델 불러오기.!\n",
    "\n",
    "DistilBertForSequenceClassification 클래스를 사용해서 사전 학습모델을 불러오세연"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): DistilBertSdpaAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "# pipeline.model(DistilBertForSequenceClassification)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TrainingARugments 설정.\n",
    "\n",
    "TrainArguments 클래스를 호출해서 파인튜닝에 사용하기 위한 하이퍼 파라미터를 정의하세요.\n",
    "\n",
    "\n",
    "딥러닝 신경망 모델에서 하이퍼 파라미터는 모델 최적화 과정에 관여하는 조절 가능한(adjustable) 파리미터를 의미합니다.\n",
    "\n",
    "하이퍼파라미터 값을 변경함으로 모델 학습과정에 영향을 미친다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArguments(\n",
       "_n_gpu=1,\n",
       "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
       "adafactor=False,\n",
       "adam_beta1=0.9,\n",
       "adam_beta2=0.999,\n",
       "adam_epsilon=1e-08,\n",
       "auto_find_batch_size=False,\n",
       "average_tokens_across_devices=False,\n",
       "batch_eval_metrics=False,\n",
       "bf16=False,\n",
       "bf16_full_eval=False,\n",
       "data_seed=None,\n",
       "dataloader_drop_last=False,\n",
       "dataloader_num_workers=0,\n",
       "dataloader_persistent_workers=False,\n",
       "dataloader_pin_memory=True,\n",
       "dataloader_prefetch_factor=None,\n",
       "ddp_backend=None,\n",
       "ddp_broadcast_buffers=None,\n",
       "ddp_bucket_cap_mb=None,\n",
       "ddp_find_unused_parameters=None,\n",
       "ddp_timeout=1800,\n",
       "debug=[],\n",
       "deepspeed=None,\n",
       "disable_tqdm=False,\n",
       "dispatch_batches=None,\n",
       "do_eval=False,\n",
       "do_predict=True,\n",
       "do_train=True,\n",
       "eval_accumulation_steps=None,\n",
       "eval_delay=0,\n",
       "eval_do_concat_batches=True,\n",
       "eval_on_start=False,\n",
       "eval_steps=None,\n",
       "eval_strategy=IntervalStrategy.NO,\n",
       "eval_use_gather_object=False,\n",
       "evaluation_strategy=None,\n",
       "fp16=False,\n",
       "fp16_backend=auto,\n",
       "fp16_full_eval=False,\n",
       "fp16_opt_level=O1,\n",
       "fsdp=[],\n",
       "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
       "fsdp_min_num_params=0,\n",
       "fsdp_transformer_layer_cls_to_wrap=None,\n",
       "full_determinism=False,\n",
       "gradient_accumulation_steps=1,\n",
       "gradient_checkpointing=False,\n",
       "gradient_checkpointing_kwargs=None,\n",
       "greater_is_better=None,\n",
       "group_by_length=False,\n",
       "half_precision_backend=auto,\n",
       "hub_always_push=False,\n",
       "hub_model_id=None,\n",
       "hub_private_repo=None,\n",
       "hub_strategy=HubStrategy.EVERY_SAVE,\n",
       "hub_token=<HUB_TOKEN>,\n",
       "ignore_data_skip=False,\n",
       "include_for_metrics=[],\n",
       "include_inputs_for_metrics=False,\n",
       "include_num_input_tokens_seen=False,\n",
       "include_tokens_per_second=False,\n",
       "jit_mode_eval=False,\n",
       "label_names=None,\n",
       "label_smoothing_factor=0.0,\n",
       "learning_rate=5e-05,\n",
       "length_column_name=length,\n",
       "load_best_model_at_end=False,\n",
       "local_rank=0,\n",
       "log_level=passive,\n",
       "log_level_replica=warning,\n",
       "log_on_each_node=True,\n",
       "logging_dir=working_dir/runs/Feb18_11-26-37_DESKTOP-B0QHPAD,\n",
       "logging_first_step=False,\n",
       "logging_nan_inf_filter=True,\n",
       "logging_steps=500,\n",
       "logging_strategy=IntervalStrategy.STEPS,\n",
       "lr_scheduler_kwargs={},\n",
       "lr_scheduler_type=SchedulerType.LINEAR,\n",
       "max_grad_norm=1.0,\n",
       "max_steps=-1,\n",
       "metric_for_best_model=None,\n",
       "mp_parameters=,\n",
       "neftune_noise_alpha=None,\n",
       "no_cuda=False,\n",
       "num_train_epochs=8,\n",
       "optim=OptimizerNames.ADAMW_HF,\n",
       "optim_args=None,\n",
       "optim_target_modules=None,\n",
       "output_dir=working_dir,\n",
       "overwrite_output_dir=False,\n",
       "past_index=-1,\n",
       "per_device_eval_batch_size=64,\n",
       "per_device_train_batch_size=16,\n",
       "prediction_loss_only=False,\n",
       "push_to_hub=False,\n",
       "push_to_hub_model_id=None,\n",
       "push_to_hub_organization=None,\n",
       "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
       "ray_scope=last,\n",
       "remove_unused_columns=True,\n",
       "report_to=[],\n",
       "restore_callback_states_from_checkpoint=False,\n",
       "resume_from_checkpoint=None,\n",
       "run_name=working_dir,\n",
       "save_on_each_node=False,\n",
       "save_only_model=False,\n",
       "save_safetensors=True,\n",
       "save_steps=500,\n",
       "save_strategy=SaveStrategy.STEPS,\n",
       "save_total_limit=None,\n",
       "seed=42,\n",
       "skip_memory_metrics=True,\n",
       "split_batches=None,\n",
       "tf32=None,\n",
       "torch_compile=False,\n",
       "torch_compile_backend=None,\n",
       "torch_compile_mode=None,\n",
       "torch_empty_cache_steps=None,\n",
       "torchdynamo=None,\n",
       "tpu_metrics_debug=False,\n",
       "tpu_num_cores=None,\n",
       "use_cpu=False,\n",
       "use_ipex=False,\n",
       "use_legacy_prediction_loop=False,\n",
       "use_liger_kernel=False,\n",
       "use_mps_device=False,\n",
       "warmup_ratio=0.0,\n",
       "warmup_steps=0,\n",
       "weight_decay=0,\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments('working_dir')\n",
    "args = args.set_training(num_epochs=8, batch_size=16)\n",
    "args = args.set_optimizer('adamw_torch')\n",
    "args = args.set_testing(batch_size=64)\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArguments(\n",
       "_n_gpu=1,\n",
       "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
       "adafactor=False,\n",
       "adam_beta1=0.9,\n",
       "adam_beta2=0.999,\n",
       "adam_epsilon=1e-08,\n",
       "auto_find_batch_size=False,\n",
       "average_tokens_across_devices=False,\n",
       "batch_eval_metrics=False,\n",
       "bf16=False,\n",
       "bf16_full_eval=False,\n",
       "data_seed=None,\n",
       "dataloader_drop_last=False,\n",
       "dataloader_num_workers=0,\n",
       "dataloader_persistent_workers=False,\n",
       "dataloader_pin_memory=True,\n",
       "dataloader_prefetch_factor=None,\n",
       "ddp_backend=None,\n",
       "ddp_broadcast_buffers=None,\n",
       "ddp_bucket_cap_mb=None,\n",
       "ddp_find_unused_parameters=None,\n",
       "ddp_timeout=1800,\n",
       "debug=[],\n",
       "deepspeed=None,\n",
       "disable_tqdm=False,\n",
       "dispatch_batches=None,\n",
       "do_eval=False,\n",
       "do_predict=False,\n",
       "do_train=False,\n",
       "eval_accumulation_steps=None,\n",
       "eval_delay=0,\n",
       "eval_do_concat_batches=True,\n",
       "eval_on_start=False,\n",
       "eval_steps=None,\n",
       "eval_strategy=IntervalStrategy.NO,\n",
       "eval_use_gather_object=False,\n",
       "evaluation_strategy=None,\n",
       "fp16=False,\n",
       "fp16_backend=auto,\n",
       "fp16_full_eval=False,\n",
       "fp16_opt_level=O1,\n",
       "fsdp=[],\n",
       "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
       "fsdp_min_num_params=0,\n",
       "fsdp_transformer_layer_cls_to_wrap=None,\n",
       "full_determinism=False,\n",
       "gradient_accumulation_steps=1,\n",
       "gradient_checkpointing=False,\n",
       "gradient_checkpointing_kwargs=None,\n",
       "greater_is_better=None,\n",
       "group_by_length=False,\n",
       "half_precision_backend=auto,\n",
       "hub_always_push=False,\n",
       "hub_model_id=None,\n",
       "hub_private_repo=None,\n",
       "hub_strategy=HubStrategy.EVERY_SAVE,\n",
       "hub_token=<HUB_TOKEN>,\n",
       "ignore_data_skip=False,\n",
       "include_for_metrics=[],\n",
       "include_inputs_for_metrics=False,\n",
       "include_num_input_tokens_seen=False,\n",
       "include_tokens_per_second=False,\n",
       "jit_mode_eval=False,\n",
       "label_names=None,\n",
       "label_smoothing_factor=0.0,\n",
       "learning_rate=5e-05,\n",
       "length_column_name=length,\n",
       "load_best_model_at_end=False,\n",
       "local_rank=0,\n",
       "log_level=passive,\n",
       "log_level_replica=warning,\n",
       "log_on_each_node=True,\n",
       "logging_dir=./logs,\n",
       "logging_first_step=False,\n",
       "logging_nan_inf_filter=True,\n",
       "logging_steps=10,\n",
       "logging_strategy=IntervalStrategy.STEPS,\n",
       "lr_scheduler_kwargs={},\n",
       "lr_scheduler_type=SchedulerType.LINEAR,\n",
       "max_grad_norm=1.0,\n",
       "max_steps=-1,\n",
       "metric_for_best_model=None,\n",
       "mp_parameters=,\n",
       "neftune_noise_alpha=None,\n",
       "no_cuda=False,\n",
       "num_train_epochs=8,\n",
       "optim=OptimizerNames.ADAMW_TORCH,\n",
       "optim_args=None,\n",
       "optim_target_modules=None,\n",
       "output_dir=./results,\n",
       "overwrite_output_dir=False,\n",
       "past_index=-1,\n",
       "per_device_eval_batch_size=64,\n",
       "per_device_train_batch_size=16,\n",
       "prediction_loss_only=False,\n",
       "push_to_hub=False,\n",
       "push_to_hub_model_id=None,\n",
       "push_to_hub_organization=None,\n",
       "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
       "ray_scope=last,\n",
       "remove_unused_columns=True,\n",
       "report_to=[],\n",
       "restore_callback_states_from_checkpoint=False,\n",
       "resume_from_checkpoint=None,\n",
       "run_name=./results,\n",
       "save_on_each_node=False,\n",
       "save_only_model=False,\n",
       "save_safetensors=True,\n",
       "save_steps=500,\n",
       "save_strategy=SaveStrategy.STEPS,\n",
       "save_total_limit=None,\n",
       "seed=42,\n",
       "skip_memory_metrics=True,\n",
       "split_batches=None,\n",
       "tf32=None,\n",
       "torch_compile=False,\n",
       "torch_compile_backend=None,\n",
       "torch_compile_mode=None,\n",
       "torch_empty_cache_steps=None,\n",
       "torchdynamo=None,\n",
       "tpu_metrics_debug=False,\n",
       "tpu_num_cores=None,\n",
       "use_cpu=False,\n",
       "use_ipex=False,\n",
       "use_legacy_prediction_loop=False,\n",
       "use_liger_kernel=False,\n",
       "use_mps_device=False,\n",
       "warmup_ratio=0.0,\n",
       "warmup_steps=500,\n",
       "weight_decay=0.01,\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = './results',\n",
    "    num_train_epochs=8,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10\n",
    ")\n",
    "\n",
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "model = model.to(device)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer 클래스 사전학습\n",
    "\n",
    "문제 004 부터 011까지의 과정에 기반한 Trainer 클래스를 인스턴스화 하고 파인튜닝을 하세요.\n",
    "\n",
    "키워드 인수로 model, args, train_dataset, eval_dataset을 명확하게 전달하세요.\n",
    "\n",
    "그리고 파인튜닝 전후에 다음 세 문장 각각의 극성(긍정/부정)판별 결과를 비교하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['positive', 'positive', 'negative']\n"
     ]
    }
   ],
   "source": [
    "# 파인튜닝 이전 세 입력 문장 극성 판별\n",
    "# 토크 나이징.\n",
    "input_tokens = tokenizer([\"I feel fantastic\", \"My life is going something wrong\",\n",
    "                          \"I have not figured out what the chosen title has to do with the move.\"],\n",
    "                          truncation=True,\n",
    "                          padding=True)\n",
    "\n",
    "# 입력 문장 토크나이징 결과(input_tokens)에 담긴 input_ids를 모델에 투입\n",
    "# 그리고 모델 출력 결과를 GPU로 전송하며 값은 변수 outputs에 저장\n",
    "outputs = model(torch.tensor(input_tokens['input_ids']).to(device))\n",
    "\n",
    "# Result 딕셔너리\n",
    "label_dict = {0: 'positive', 1: 'negative'}\n",
    "\n",
    "# outputs 변수에 담긴 logits 값을 행 단위, 즉 입력 문장 단위로 가장 큰 값의 위치(인덱스) 추출\n",
    "# 껼과값(인덱스)을 CPU로 넘기고 넘파이 타입으로 변경 후, 인덱스에 매칭되는 레이블 출격\n",
    "\n",
    "print([label_dict[i] for i in torch.argmax(outputs['logits'], axis=1).cpu().numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='400' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [400/400 00:12, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.719300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.685800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.619800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.489100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.290800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.141000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.064500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.029900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.017400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.011200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.008000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.006100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.004600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.003700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.003000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.002600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.002200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.001800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.001600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=400, training_loss=0.0777999286938575, metrics={'train_runtime': 13.6008, 'train_samples_per_second': 470.56, 'train_steps_per_second': 29.41, 'total_flos': 4967527449600.0, 'train_loss': 0.0777999286938575, 'epoch': 8.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = val_dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['positive', 'positive', 'positive']\n"
     ]
    }
   ],
   "source": [
    "# 파인튜닝 이전 세 입력 문장 극성 판별\n",
    "# 토크 나이징.\n",
    "input_tokens = tokenizer([\"I feel fantastic\", \"My life is going something wrong\",\n",
    "                          \"I have not figured out what the chosen title has to do with the move.\"],\n",
    "                          truncation=True,\n",
    "                          padding=True)\n",
    "\n",
    "# 입력 문장 토크나이징 결과(input_tokens)에 담긴 input_ids를 모델에 투입\n",
    "# 그리고 모델 출력 결과를 GPU로 전송하며 값은 변수 outputs에 저장\n",
    "outputs = model(torch.tensor(input_tokens['input_ids']).to(device))\n",
    "\n",
    "# Result 딕셔너리\n",
    "label_dict = {0: 'positive', 1: 'negative'}\n",
    "\n",
    "# outputs 변수에 담긴 logits 값을 행 단위, 즉 입력 문장 단위로 가장 큰 값의 위치(인덱스) 추출\n",
    "# 껼과값(인덱스)을 CPU로 넘기고 넘파이 타입으로 변경 후, 인덱스에 매칭되는 레이블 출격\n",
    "\n",
    "print([label_dict[i] for i in torch.argmax(outputs['logits'], axis=1).cpu().numpy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 파이토치 사전학습\n",
    "\n",
    "문제 004부터 011까지에 기반한 모델을 파인튜닝 하기 위해서 파이토치를 사용하세요.\n",
    "\n",
    "문제 012에서는 Trainer 클래스를 사용헀지만 여기서는 파이토치를 사용합니다.\n",
    "\n",
    "이후 파인튜닝 전 후에 다음 세문장 각각의 극성 판별결과를 비교하시오.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파이토치 파인튜닝 과정\n",
    "\n",
    "1. 사전 학습 모델 및 토크나이저 불러오기\n",
    "2. DataLoader 인스턴스화\n",
    "3. 최적함수 정의\n",
    "4. 모델을 학습(training)모드로 전환\n",
    "5. 에포크 횟수만큼 반복\n",
    "6. 최적화 함수의 그래디언트(기울기) 초기화\n",
    "7. 모델을 사용한 추론\n",
    "8. 손실 계산\n",
    "9. 오차역전파\n",
    "10. 가중치 업데이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_inference(model, tokenizer):\n",
    "    input_tokens = tokenizer([\"I feel fantastic\", \"My life is going something wrong\",\n",
    "                            \"I have not figured out what the chosen title has to do with the move.\"],\n",
    "                            truncation=True,\n",
    "                            padding=True)\n",
    "\n",
    "    outputs = model(torch.tensor(input_tokens['input_ids']).to(device))\n",
    "\n",
    "    # Result 딕셔너리\n",
    "    label_dict = {0: 'positive', 1: 'negative'}\n",
    "\n",
    "\n",
    "    return [label_dict[i] for i in torch.argmax(outputs['logits'], axis=1).cpu().numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/mnt/d/workspace/colab_try/.venv/lib/python3.12/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['positive', 'negative', 'positive']\n",
      "epoch: 0 <torch.utils.data.dataloader.DataLoader object at 0x7fd6fe042600>\n",
      "epoch: 1 <torch.utils.data.dataloader.DataLoader object at 0x7fd6fe042600>\n",
      "epoch: 2 <torch.utils.data.dataloader.DataLoader object at 0x7fd6fe042600>\n",
      "epoch: 3 <torch.utils.data.dataloader.DataLoader object at 0x7fd6fe042600>\n",
      "epoch: 4 <torch.utils.data.dataloader.DataLoader object at 0x7fd6fe042600>\n",
      "epoch: 5 <torch.utils.data.dataloader.DataLoader object at 0x7fd6fe042600>\n",
      "epoch: 6 <torch.utils.data.dataloader.DataLoader object at 0x7fd6fe042600>\n",
      "epoch: 7 <torch.utils.data.dataloader.DataLoader object at 0x7fd6fe042600>\n",
      "['positive', 'positive', 'positive']\n"
     ]
    }
   ],
   "source": [
    "# 10단계\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DistilBertForSequenceClassification, AdamW\n",
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "\n",
    "# 1) 사전학습 모델과 토크나이저 불러오기\n",
    "# 그리고 모델 실행결과에 to(device) 코드 추가\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "model.to(device)\n",
    "\n",
    "print(test_inference(model, tokenizer))\n",
    "\n",
    "# 2)DataLoader 인스턴스화\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# 3)최적화 함수 정의\n",
    "optim = AdamW(model.parameters(), lr=5e-5)  # 0.0005\n",
    "\n",
    "# 4)모델을 학습(Train) 모드로 전환\n",
    "# 이는 드롭아웃 및 배치 정규화에 영향을 미침\n",
    "model.train()\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(8):\n",
    "    print(f'epoch: {epoch}')\n",
    "    for batch in train_loader:\n",
    "        # 6)최적화 함수의 기울기(그래디언트) 초기화\n",
    "        print(batch, '@@')\n",
    "        optim.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # 7)모델을 사용한 추론\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "        # 8) 손실계산\n",
    "        loss = outputs[0]\n",
    "        losses.append(loss)\n",
    "\n",
    "        # 9)오차역전파\n",
    "        loss.backward()\n",
    "\n",
    "        # 10)가중치(weight) 업데이트\n",
    "        optim.step()\n",
    "\n",
    "# 모델을 eval 모드로 전환\n",
    "model.eval()\n",
    "print(test_inference(model, tokenizer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
